{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ian/venvs/tf2.0/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "# import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/ian/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/ian/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/ian/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/ian/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Size of:\n",
      "- Training-set:\t\t55000\n",
      "- Validation-set:\t5000\n"
     ]
    }
   ],
   "source": [
    "folder = \"/Users/ian/MNIST_data/\"\n",
    "img_h = img_w = 28             # MNIST images are 28x28\n",
    "img_size_flat = img_h * img_w  # 28x28=784, the total number of pixels\n",
    "n_classes = 10                 # Number of classes, one class per digit\n",
    " \n",
    "def load_data(mode='train'):\n",
    "    \"\"\"\n",
    "    Function to (download and) load the MNIST data\n",
    "    :param mode: train or test\n",
    "    :return: images and the corresponding labels\n",
    "    \"\"\"\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    mnist = input_data.read_data_sets(folder, one_hot=True)\n",
    "    if mode == 'train':\n",
    "        x_train, y_train, x_valid, y_valid = mnist.train.images, mnist.train.labels, \\\n",
    "                                             mnist.validation.images, mnist.validation.labels\n",
    "        return x_train, y_train, x_valid, y_valid\n",
    "    elif mode == 'test':\n",
    "        x_test, y_test = mnist.test.images, mnist.test.labels\n",
    "    return x_test, y_test\n",
    "\n",
    "def randomize(x, y):\n",
    "    \"\"\" Randomizes the order of data samples and their corresponding labels\"\"\"\n",
    "    permutation = np.random.permutation(y.shape[0])\n",
    "    shuffled_x = x[permutation, :]\n",
    "    shuffled_y = y[permutation]\n",
    "    return shuffled_x, shuffled_y\n",
    "\n",
    "def get_next_batch(x, y, start, end):\n",
    "    x_batch = x[start:end]\n",
    "    y_batch = y[start:end]\n",
    "    return x_batch, y_batch\n",
    "# Load MNIST data\n",
    "x_train, y_train, x_valid, y_valid = load_data(mode='train')\n",
    "print(\"Size of:\")\n",
    "print(\"- Training-set:\\t\\t{}\".format(len(y_train)))\n",
    "print(\"- Validation-set:\\t{}\".format(len(y_valid)))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "epochs = 10             # Total number of training epochs\n",
    "batch_size = 100        # Training batch size\n",
    "display_freq = 100      # Frequency of displaying the training results\n",
    "learning_rate = 0.001   # The optimization initial learning rate\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = 200                # Number of units in the first hidden layer\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight and bais wrappers\n",
    "def weight_variable(name, shape):\n",
    "    \"\"\"\n",
    "    Create a weight variable with appropriate initialization\n",
    "    :param name: weight name\n",
    "    :param shape: weight shape\n",
    "    :return: initialized weight variable\n",
    "    \"\"\"\n",
    "    initer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "    return tf.get_variable('W_' + name,\n",
    "                           dtype=tf.float32,\n",
    "                           shape=shape,\n",
    "                           initializer=initer)\n",
    "\n",
    "def bias_variable(name, shape):\n",
    "    \"\"\"\n",
    "    Create a bias variable with appropriate initialization\n",
    "    :param name: bias variable name\n",
    "    :param shape: bias variable shape\n",
    "    :return: initialized bias variable\n",
    "    \"\"\"\n",
    "    initial = tf.constant(0., shape=shape, dtype=tf.float32)\n",
    "    return tf.get_variable('b_' + name,\n",
    "                           dtype=tf.float32,\n",
    "                           initializer=initial)\n",
    " \n",
    "def fc_layer(x, num_units, name, use_relu=True):\n",
    "    \"\"\"\n",
    "    Create a fully-connected layer\n",
    "    :param x: input from previous layer\n",
    "    :param num_units: number of hidden units in the fully-connected layer\n",
    "    :param name: layer name\n",
    "    :param use_relu: boolean to add ReLU non-linearity (or not)\n",
    "    :return: The output array\n",
    "    \"\"\"\n",
    "    in_dim = x.get_shape()[1]\n",
    "    W = weight_variable(name, shape=[in_dim, num_units])\n",
    "    b = bias_variable(name, [num_units])\n",
    "    layer = tf.matmul(x, W)\n",
    "    layer += b\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "    return layer\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph for the linear model\n",
    "# Placeholders for inputs (x) and outputs(y)\n",
    "x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='X')\n",
    "y = tf.placeholder(tf.float32, shape=[None, n_classes], name='Y')\n",
    "fc1 = fc_layer(x, h1, 'FC1', use_relu=True)\n",
    "output_logits = fc_layer(fc1, n_classes, 'OUT', use_relu=False)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-16-3d235b168752>:5: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Network predictions\n",
    "cls_prediction = tf.argmax(output_logits, axis=1, name='predictions')\n",
    "\n",
    "# Define the loss function, optimizer, and accuracy\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_logits), name='loss')\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='Adam-op').minimize(loss)\n",
    "correct_prediction = tf.equal(tf.argmax(output_logits, 1), tf.argmax(y, 1), name='correct_pred')\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the op for initializing all variables\n",
    "init = tf.global_variables_initializer()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "iter   0:\t Loss=2.28,\tTraining Accuracy=35.0%\n",
      "iter 100:\t Loss=0.39,\tTraining Accuracy=87.0%\n",
      "iter 200:\t Loss=0.24,\tTraining Accuracy=90.0%\n",
      "iter 300:\t Loss=0.51,\tTraining Accuracy=85.0%\n",
      "iter 400:\t Loss=0.29,\tTraining Accuracy=96.0%\n",
      "iter 500:\t Loss=0.41,\tTraining Accuracy=86.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 1, validation loss: 0.24, validation accuracy: 92.5%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 2\n",
      "iter   0:\t Loss=0.27,\tTraining Accuracy=93.0%\n",
      "iter 100:\t Loss=0.18,\tTraining Accuracy=93.0%\n",
      "iter 200:\t Loss=0.19,\tTraining Accuracy=93.0%\n",
      "iter 300:\t Loss=0.09,\tTraining Accuracy=98.0%\n",
      "iter 400:\t Loss=0.18,\tTraining Accuracy=93.0%\n",
      "iter 500:\t Loss=0.12,\tTraining Accuracy=97.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 2, validation loss: 0.17, validation accuracy: 95.6%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 3\n",
      "iter   0:\t Loss=0.07,\tTraining Accuracy=98.0%\n",
      "iter 100:\t Loss=0.09,\tTraining Accuracy=98.0%\n",
      "iter 200:\t Loss=0.11,\tTraining Accuracy=95.0%\n",
      "iter 300:\t Loss=0.21,\tTraining Accuracy=95.0%\n",
      "iter 400:\t Loss=0.07,\tTraining Accuracy=97.0%\n",
      "iter 500:\t Loss=0.15,\tTraining Accuracy=96.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 3, validation loss: 0.14, validation accuracy: 96.2%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 4\n",
      "iter   0:\t Loss=0.10,\tTraining Accuracy=98.0%\n",
      "iter 100:\t Loss=0.12,\tTraining Accuracy=97.0%\n",
      "iter 200:\t Loss=0.09,\tTraining Accuracy=97.0%\n",
      "iter 300:\t Loss=0.15,\tTraining Accuracy=95.0%\n",
      "iter 400:\t Loss=0.06,\tTraining Accuracy=99.0%\n",
      "iter 500:\t Loss=0.07,\tTraining Accuracy=98.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 4, validation loss: 0.11, validation accuracy: 96.9%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 5\n",
      "iter   0:\t Loss=0.06,\tTraining Accuracy=98.0%\n",
      "iter 100:\t Loss=0.11,\tTraining Accuracy=98.0%\n",
      "iter 200:\t Loss=0.06,\tTraining Accuracy=99.0%\n",
      "iter 300:\t Loss=0.04,\tTraining Accuracy=98.0%\n",
      "iter 400:\t Loss=0.12,\tTraining Accuracy=98.0%\n",
      "iter 500:\t Loss=0.03,\tTraining Accuracy=99.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 5, validation loss: 0.10, validation accuracy: 97.3%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 6\n",
      "iter   0:\t Loss=0.11,\tTraining Accuracy=96.0%\n",
      "iter 100:\t Loss=0.03,\tTraining Accuracy=99.0%\n",
      "iter 200:\t Loss=0.04,\tTraining Accuracy=99.0%\n",
      "iter 300:\t Loss=0.06,\tTraining Accuracy=97.0%\n",
      "iter 400:\t Loss=0.02,\tTraining Accuracy=100.0%\n",
      "iter 500:\t Loss=0.03,\tTraining Accuracy=100.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 6, validation loss: 0.10, validation accuracy: 97.3%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 7\n",
      "iter   0:\t Loss=0.01,\tTraining Accuracy=100.0%\n",
      "iter 100:\t Loss=0.02,\tTraining Accuracy=100.0%\n",
      "iter 200:\t Loss=0.07,\tTraining Accuracy=99.0%\n",
      "iter 300:\t Loss=0.01,\tTraining Accuracy=100.0%\n",
      "iter 400:\t Loss=0.03,\tTraining Accuracy=99.0%\n",
      "iter 500:\t Loss=0.04,\tTraining Accuracy=99.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 7, validation loss: 0.09, validation accuracy: 97.3%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 8\n",
      "iter   0:\t Loss=0.02,\tTraining Accuracy=100.0%\n",
      "iter 100:\t Loss=0.03,\tTraining Accuracy=99.0%\n",
      "iter 200:\t Loss=0.01,\tTraining Accuracy=100.0%\n",
      "iter 300:\t Loss=0.04,\tTraining Accuracy=99.0%\n",
      "iter 400:\t Loss=0.03,\tTraining Accuracy=100.0%\n",
      "iter 500:\t Loss=0.04,\tTraining Accuracy=99.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 8, validation loss: 0.10, validation accuracy: 96.7%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 9\n",
      "iter   0:\t Loss=0.02,\tTraining Accuracy=100.0%\n",
      "iter 100:\t Loss=0.02,\tTraining Accuracy=100.0%\n",
      "iter 200:\t Loss=0.05,\tTraining Accuracy=99.0%\n",
      "iter 300:\t Loss=0.01,\tTraining Accuracy=100.0%\n",
      "iter 400:\t Loss=0.01,\tTraining Accuracy=100.0%\n",
      "iter 500:\t Loss=0.02,\tTraining Accuracy=98.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 9, validation loss: 0.09, validation accuracy: 97.2%\n",
      "---------------------------------------------------------\n",
      "Training epoch: 10\n",
      "iter   0:\t Loss=0.00,\tTraining Accuracy=100.0%\n",
      "iter 100:\t Loss=0.01,\tTraining Accuracy=100.0%\n",
      "iter 200:\t Loss=0.01,\tTraining Accuracy=100.0%\n",
      "iter 300:\t Loss=0.03,\tTraining Accuracy=99.0%\n",
      "iter 400:\t Loss=0.01,\tTraining Accuracy=100.0%\n",
      "iter 500:\t Loss=0.01,\tTraining Accuracy=100.0%\n",
      "---------------------------------------------------------\n",
      "Epoch: 10, validation loss: 0.09, validation accuracy: 97.7%\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "global_step = 0\n",
    "# Number of training iterations in each epoch\n",
    "num_tr_iter = int(len(y_train) / batch_size)\n",
    "for epoch in range(epochs):\n",
    "    print('Training epoch: {}'.format(epoch + 1))\n",
    "    x_train, y_train = randomize(x_train, y_train)\n",
    "    for iteration in range(num_tr_iter):\n",
    "        global_step += 1\n",
    "        start = iteration * batch_size\n",
    "        end = (iteration + 1) * batch_size\n",
    "        x_batch, y_batch = get_next_batch(x_train, y_train, start, end)\n",
    "\n",
    "        # Run optimization op (backprop)\n",
    "        feed_dict_batch = {x: x_batch, y: y_batch}\n",
    "        sess.run(optimizer, feed_dict=feed_dict_batch)\n",
    "\n",
    "        if iteration % display_freq == 0:\n",
    "            # Calculate and display the batch loss and accuracy\n",
    "            loss_batch, acc_batch = sess.run([loss, accuracy],\n",
    "                                             feed_dict=feed_dict_batch)\n",
    "\n",
    "            print(\"iter {0:3d}:\\t Loss={1:.2f},\\tTraining Accuracy={2:.01%}\".\n",
    "                  format(iteration, loss_batch, acc_batch))\n",
    "\n",
    "    # Run validation after every epoch\n",
    "    feed_dict_valid = {x: x_valid[:1000], y: y_valid[:1000]}\n",
    "    loss_valid, acc_valid = sess.run([loss, accuracy], feed_dict=feed_dict_valid)\n",
    "    print('---------------------------------------------------------')\n",
    "    print(\"Epoch: {0}, validation loss: {1:.2f}, validation accuracy: {2:.01%}\".\n",
    "          format(epoch + 1, loss_valid, acc_valid))\n",
    "    print('---------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, cls_true, cls_pred=None, title=None):\n",
    "    \"\"\"\n",
    "    Create figure with 3x3 sub-plots.\n",
    "    :param images: array of images to be plotted, (9, img_h*img_w)\n",
    "    :param cls_true: corresponding true labels (9,)\n",
    "    :param cls_pred: corresponding true labels (9,)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(9, 9))\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(28, 28), cmap='binary')\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            ax_title = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            ax_title = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        ax.set_title(ax_title)\n",
    "\n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    if title:\n",
    "        plt.suptitle(title, size=20)\n",
    "    plt.show(block=False)\n",
    "\n",
    "def plot_example_errors(images, cls_true, cls_pred, title=None):\n",
    "    \"\"\"\n",
    "    Function for plotting examples of images that have been mis-classified\n",
    "    :param images: array of all images, (#imgs, img_h*img_w)\n",
    "    :param cls_true: corresponding true labels, (#imgs,)\n",
    "    :param cls_pred: corresponding predicted labels, (#imgs,)\n",
    "    \"\"\"\n",
    "    # Negate the boolean array.\n",
    "    incorrect = np.logical_not(np.equal(cls_pred, cls_true))\n",
    "\n",
    "    # Get the images from the test-set that have been\n",
    "    # incorrectly classified.\n",
    "    incorrect_images = images[incorrect]\n",
    "\n",
    "    # Get the true and predicted classes for those images.\n",
    "    cls_pred = cls_pred[incorrect]\n",
    "    cls_true = cls_true[incorrect]\n",
    "\n",
    "    # Plot the first 9 images.\n",
    "    plot_images(images=incorrect_images[0:9],\n",
    "                cls_true=cls_true[0:9],\n",
    "                cls_pred=cls_pred[0:9],\n",
    "                title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the network after training\n",
    "x_test, y_test = load_data(mode='test')\n",
    "feed_dict_test = {x: x_test[:1000], y: y_test[:1000]}\n",
    "loss_test, acc_test = sess.run([loss, accuracy], feed_dict=feed_dict_test)\n",
    "print('---------------------------------------------------------')\n",
    "print(\"Test loss: {0:.2f}, test accuracy: {1:.01%}\".format(loss_test, acc_test))\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Plot some of the correct and misclassified examples\n",
    "cls_pred = sess.run(cls_prediction, feed_dict=feed_dict_test)\n",
    "cls_true = np.argmax(y_test[:1000], axis=1)\n",
    "plot_images(x_test, cls_true, cls_pred, title='Correct Examples')\n",
    "plot_example_errors(x_test[:1000], cls_true, cls_pred, title='Misclassified Examples')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
